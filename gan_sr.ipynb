{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8a0745-8579-49e3-b656-c18fed566163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5054df-cc68-4f2f-a7cc-311b0c281219",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  \n",
    "lr_v = 2e-4\n",
    "tot_sample= 100  # Totall traning images\n",
    "## adversarial learning (SRGAN)\n",
    "n_epoch = 500\n",
    "## initialize G\n",
    "n_epoch_init = n_epoch//10\n",
    "\n",
    "# create folders to save result images and trained models\n",
    "save_dir = \"samples\"\n",
    "checkpoint_dir = \"models\"\n",
    "#track image as per index\n",
    "save_ind= 16\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd1336a-2a65-4ab2-83b7-881fedba443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path,shape):\n",
    "    img= cv2.imread(path)\n",
    "    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img= cv2.resize(img, shape)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_data(path):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for folder in glob.glob(path+ str('/*')):\n",
    "        for img_path in glob.glob(folder+ str('/*')):      \n",
    "            if folder == os.path.join(path, 'HR'):\n",
    "                X.append(load(img_path, (384, 384)))\n",
    "            elif folder == os.path.join(path, 'LR'):\n",
    "                Y.append(load(img_path, (96,96)))\n",
    "\n",
    "    X= np.array(X)\n",
    "    Y= np.array(Y)\n",
    "    return X/255.0, Y/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5400d7ad-8d5b-4332-9272-222f8ac750e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 384, 384, 3), (100, 96, 96, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HR_train, LR_train= get_data('Dataset')\n",
    "HR_train.shape, LR_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226e4ca-2233-4589-906b-31893153f492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a978ac6070>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(HR_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a7ebc-5036-4217-bf06-cf33b5a2282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax= plt.subplots(1,2, figsize=(16, 6))\n",
    "ax[0].imshow(LR_train[save_ind], aspect='auto')\n",
    "ax[1].imshow(HR_train[save_ind], aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea03e57-e89b-4c6a-87bd-ec3c8189b1d6",
   "metadata": {},
   "source": [
    "GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc458c94-91b9-49d8-8230-886edced8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, add,\\\n",
    "                                    BatchNormalization, Activation, LeakyReLU, Layer\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca57ef6-405d-4fdc-8ac3-fdb8bb4ba6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubpixelConv2D(Layer):\n",
    "    \"\"\" Subpixel Conv2D Layer\n",
    "    upsampling a layer from (h, w, c) to (h*r, w*r, c/(r*r)),\n",
    "    where r is the scaling factor, default to 4\n",
    "    # Arguments\n",
    "    upsampling_factor: the scaling factor\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        the second and the third dimension increased by a factor of\n",
    "        `upsampling_factor`; the last layer decreased by a factor of\n",
    "        `upsampling_factor^2`.\n",
    "    # References\n",
    "        Real-Time Single Image and Video Super-Resolution Using an Efficient\n",
    "        Sub-Pixel Convolutional Neural Network Shi et Al. https://arxiv.org/abs/1609.05158\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, upsampling_factor=2, **kwargs):\n",
    "        super(SubpixelConv2D, self).__init__(**kwargs)\n",
    "        self.upsampling_factor = upsampling_factor\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        last_dim = input_shape[-1]\n",
    "        factor = self.upsampling_factor * self.upsampling_factor\n",
    "        if last_dim % (factor) != 0:\n",
    "            raise ValueError('Channel ' + str(last_dim) + ' should be of '\n",
    "                             'integer times of upsampling_factor^2: ' +\n",
    "                             str(factor) + '.')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.nn.depth_to_space( inputs, self.upsampling_factor )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = { 'upsampling_factor': self.upsampling_factor, }\n",
    "        base_config = super(SubpixelConv2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        factor = self.upsampling_factor * self.upsampling_factor\n",
    "        input_shape_1 = None\n",
    "        if input_shape[1] is not None:\n",
    "            input_shape_1 = input_shape[1] * self.upsampling_factor\n",
    "        input_shape_2 = None\n",
    "        if input_shape[2] is not None:\n",
    "            input_shape_2 = input_shape[2] * self.upsampling_factor\n",
    "        dims = [ input_shape[0],\n",
    "                 input_shape_1,\n",
    "                 input_shape_2,\n",
    "                 int(input_shape[3]/factor)\n",
    "               ]\n",
    "        return tuple( dims )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cd77f-ce17-4b41-b7e9-199f00eeade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_G(input_shape):\n",
    "    # w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    g_init = tf.random_normal_initializer(1., 0.02)\n",
    "    relu= Activation('relu')\n",
    "\n",
    "    nin= Input(shape= input_shape)\n",
    "    n= Conv2D(64, (3,3), padding='SAME', activation= 'relu',\n",
    "                        kernel_initializer='HeNormal')(nin)\n",
    "    temp= n\n",
    "\n",
    "\n",
    "    # B residual blocks\n",
    "    for i in range(3):\n",
    "        nn= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "        nn= BatchNormalization(gamma_initializer= g_init)(nn)\n",
    "        nn= relu(nn)\n",
    "        nn= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "        nn= BatchNormalization(gamma_initializer= g_init)(nn)\n",
    "\n",
    "        nn= add([n, nn])\n",
    "        n= nn\n",
    "\n",
    "    n= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
    "    n= add([n, temp])\n",
    "    # B residual blacks end\n",
    "\n",
    "    n= Conv2D(256, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "    n= SubpixelConv2D(upsampling_factor=2)(n)\n",
    "    n= relu(n)\n",
    "\n",
    "    n= Conv2D(256, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "    n= SubpixelConv2D(upsampling_factor=2)(n)\n",
    "    n= relu(n)\n",
    "\n",
    "    nn= Conv2D(3, (1,1), padding='SAME', kernel_initializer='HeNormal', activation= 'tanh')(n)\n",
    "\n",
    "\n",
    "    G = Model(inputs=nin, outputs=nn, name=\"generator\")\n",
    "    return G\n",
    "\n",
    "\n",
    "# discriminator\n",
    "def get_D(input_shape):\n",
    "\n",
    "    g_init= tf.random_normal_initializer(1., 0.02)\n",
    "    ly_relu= LeakyReLU(alpha= 0.2)\n",
    "    df_dim = 16\n",
    "\n",
    "    nin = Input(input_shape)\n",
    "    n = Conv2D(64, (4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(nin)\n",
    "    n= ly_relu(n)\n",
    "\n",
    "    for i in range(2, 6):\n",
    "        n = Conv2D(df_dim*(2**i),(4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "        n= ly_relu(n)\n",
    "        n= BatchNormalization(gamma_initializer= g_init)(n)\n",
    "\n",
    "    n= Conv2D(df_dim*16, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "    n= ly_relu(n)\n",
    "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
    "\n",
    "    n= Conv2D(df_dim*8, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
    "    temp= n\n",
    "\n",
    "    n= Conv2D(df_dim*4, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "    n= ly_relu(n)\n",
    "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
    "\n",
    "    n= Conv2D(df_dim*8, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n",
    "    n= BatchNormalization(gamma_initializer= g_init)(n)\n",
    "\n",
    "    n= add([n, temp])\n",
    "\n",
    "    n= Flatten()(n)\n",
    "    no= Dense(units=1, kernel_initializer='HeNormal', activation= 'sigmoid')(n)\n",
    "    D= Model(inputs=nin, outputs=no, name=\"discriminator\")\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "# VGG19\n",
    "def get_vgg19():\n",
    "    vgg= tf.keras.applications.VGG19( include_top=False, weights='imagenet', \n",
    "                                    input_tensor=None, input_shape=(384, 384, 3),\n",
    "                                    pooling=None, classes=1000, classifier_activation='softmax' )\n",
    "\n",
    "    inp= Input(shape=(384, 384, 3))\n",
    "    x= vgg.layers[0](inp)\n",
    "    for ly in vgg.layers[1:17]:\n",
    "        x= ly(x)\n",
    "    VGG19= Model(inp, x)\n",
    "\n",
    "    return VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe29ac6-6e53-4d3d-b4cd-3a31d0726627",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = get_G((96, 96, 3))\n",
    "D = get_D((384, 384, 3))\n",
    "vgg= get_vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffa606-f697-4896-8242-0ea42cd7e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer_init = tf.optimizers.Adam(lr_v)\n",
    "g_optimizer = tf.optimizers.Adam(lr_v)\n",
    "d_optimizer = tf.optimizers.Adam(lr_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cdbfc-2f1b-4592-870a-3da5b119fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step_epoch = round(n_epoch_init // batch_size)\n",
    "for epoch in range(n_epoch_init):\n",
    "  i,j= ((epoch)*batch_size)%tot_sample, (((epoch+1))*batch_size)%tot_sample\n",
    "  if j== 0:\n",
    "    j= -1\n",
    "  X, Y= LR_train[i: j], HR_train[i: j]\n",
    "  with tf.GradientTape() as tape:\n",
    "      ypred= G(X)\n",
    "      mse_loss= tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(Y, ypred), axis=-1))\n",
    "      grad = tape.gradient(mse_loss, G.trainable_weights)\n",
    "      g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\n",
    "        \n",
    "  print(\"Epoch: [{}/{}] step: mse: {:.3f} \".format(\n",
    "            epoch, n_epoch_init , mse_loss))\n",
    "  if epoch%10 ==0:\n",
    "    img= G.predict(LR_train[np.newaxis, save_ind])[0]\n",
    "    #img= (img-img.mean())/img.std()\n",
    "    img= Image.fromarray(np.uint8(img*255))\n",
    "    img.save(os.path.join(save_dir, 'init_g_{}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9056f-c622-4526-b4ee-4e5c00d49ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax= plt.subplots(1,5, figsize=(16, 6))\n",
    "for i, file in enumerate(glob.glob('./samples/init*')):\n",
    "    img= load(file, shape=(384, 384))\n",
    "    ax[i].imshow(img)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8fee93-03d9-4f96-9fac-694921b5da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch= n_epoch-200\n",
    "for epoch in range(n_epoch):\n",
    "        i,j= ((epoch)*batch_size)%tot_sample, (((epoch+1))*batch_size)%tot_sample\n",
    "        if j== 0:\n",
    "            j= -1\n",
    "        X, Y= LR_train[i: j], HR_train[i: j]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            fake_img= G(X)\n",
    "            fake_logits= D(fake_img)\n",
    "            real_logits= D(Y)\n",
    "            fake_feature= vgg(fake_img)\n",
    "            real_feature= vgg(Y)\n",
    "\n",
    "            #D. loss\n",
    "            d_loss1= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits , tf.zeros_like(fake_logits)))\n",
    "            d_loss2= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_logits,tf.ones_like(real_logits)))\n",
    "            d_loss= d_loss1 + d_loss2\n",
    "\n",
    "            #G. loss\n",
    "            g_gan_loss= 2e-3*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits , tf.ones_like(fake_logits)))\n",
    "            mse_loss=  2e-1* tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(fake_img, Y), axis=-1))\n",
    "            vgg_loss = 2e-6 * tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(fake_feature, real_feature), axis=-1))\n",
    "            g_loss = mse_loss + vgg_loss + g_gan_loss\n",
    "\n",
    "            grad = tape.gradient(g_loss, G.trainable_weights)\n",
    "            g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\n",
    "            grad = tape.gradient(d_loss, D.trainable_weights)\n",
    "            d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\n",
    "\n",
    "        print(\"Epoch: [{}/{}] step: D.loss: {:.3f}: G.loss: {:.3f}\".format(\n",
    "                epoch, n_epoch , d_loss, g_loss))\n",
    "\n",
    "\n",
    "        if epoch%20 ==0:\n",
    "            img= G.predict(LR_train[np.newaxis, save_ind])[0]\n",
    "            # if not sigmoid\n",
    "            #img= (img-img.mean())/img.std()\n",
    "            img= Image.fromarray(np.uint8(img*255))\n",
    "            img.save(os.path.join(save_dir, 'train_g_{}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31f25f-3d38-4d27-b799-800ada313a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax= plt.subplots(3,5, figsize=(14, 16))\n",
    "for i, file in enumerate(glob.glob('./samples/train*')[:15]):\n",
    "    img= load(file, shape=(384, 384))\n",
    "    ax[i//5][i%5].imshow(img, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987788a-6903-42a4-ab3c-9e52df4b462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax= plt.subplots(1,3, figsize=(16, 6))\n",
    "ax[0].imshow(LR_train[save_ind], aspect='auto')\n",
    "ax[1].imshow(load(glob.glob('./samples/train*')[-1], (384, 384)), aspect='auto')\n",
    "ax[2].imshow(HR_train[save_ind], aspect='auto')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
